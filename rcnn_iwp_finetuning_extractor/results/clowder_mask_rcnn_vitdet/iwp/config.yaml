cfg:
  backbone:
    norm: LN
    out_channels: 256
    scale_factors: [4.0, 2.0, 1.0, 0.5]
    square_pad: 1024
  dataloader:
    train: {total_batch_size: 1}
  input_format: RGB
  optimizer:
    lr_decay_rate: 0.7
    num_layers: 12
    params_overrides:
      pos_embed: {weight_decay: 0.0}
    type: AdamW
  roi_heads:
    batch_size_per_image: 512
    box_head:
      conv_dims: [256, 256, 256, 256]
      conv_norm: LN
      fc_dims: [1024]
      input_shape: {channels: 256, height: 7, width: 7}
    box_in_features: [p2, p3, p4, p5]
    box_pooler:
      output_size: 7
      pooler_type: ROIAlignV2
      sampling_ratio: 0
      scales: [0.25, 0.125, 0.0625, 0.03125]
    box_predictor:
      box2box_weights: [10, 10, 5, 5]
      input_shape: {channels: 1024}
      test_score_thresh: 0.05
    mask_head:
      conv_dims: [256, 256, 256, 256, 256]
      conv_norm: LN
      input_shape: {channels: 256, height: 14, width: 14}
    mask_in_features: [p2, p3, p4, p5]
    mask_pooler:
      output_size: 14
      pooler_type: ROIAlignV2
      sampling_ratio: 0
      scales: [0.25, 0.125, 0.0625, 0.03125]
    num_classes: 80
    positive_fraction: 0.25
    proposal_matcher:
      allow_low_quality_matches: false
      labels: [0, 1]
      thresholds: [0.5]
  rpn:
    anchor_generator:
      aspect_ratios: [0.5, 1.0, 2.0]
      offset: 0.0
      sizes:
      - [32]
      - [64]
      - [128]
      - [256]
      - [512]
      strides: [4, 8, 16, 32, 64]
    batch_size_per_image: 256
    box2box_transform:
      weights: [1.0, 1.0, 1.0, 1.0]
    head:
      conv_dims: [-1, -1]
      in_channels: 256
      num_anchors: 3
    in_features: [p2, p3, p4, p5, p6]
    matcher:
      allow_low_quality_matches: true
      labels: [0, -1, 1]
      thresholds: [0.3, 0.7]
    nms_thresh: 0.7
    positive_fraction: 0.5
    post_nms_topk: [1000, 1000]
    pre_nms_topk: [2000, 1000]
  scheduler:
    milestone_factors: [0.889, 0.963]
    values: [1.0, 0.1, 0.01]
    warmup_factor: 0.001
  train:
    amp: {enabled: true}
    checkpointer_period: 1000
    ddp: {fp16_compression: true}
    eval_period: 200
    init_checkpoint: detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True
    max_iter: 1005
    seed: 1129
  vit:
    depth: 12
    drop_path_rate: 0.1
    embed_dim: 768
    img_size: 1024
    mlp_ratio: 4
    num_heads: 12
    out_feature: last_feat
    patch_size: 16
    qkv_bias: true
    residual_block_indexes: []
    window_block_indexes: [0, 1, 3, 4, 6, 7, 9, 10]
    window_size: 14
constants:
  imagenet_bgr256_mean: [103.53, 116.28, 123.675]
  imagenet_bgr256_std: [1.0, 1.0, 1.0]
  imagenet_rgb256_mean: [123.675, 116.28, 103.53]
  imagenet_rgb256_std: [58.395, 57.12, 57.375]
dataloader:
  evaluator: {_target_: detectron2.evaluation.COCOEvaluator, dataset_name: '${..test.dataset.names}', max_dets_per_image: 100, output_dir: results/clowder_mask_rcnn_vitdet/iwp}
  test:
    _target_: detectron2.data.build_detection_test_loader
    dataset: {_target_: detectron2.data.get_detection_dataset_dicts, filter_empty: false, names: iwp_val}
    mapper:
      _target_: dataset_mappers.base_mapper.BaseMapper
      augmentations:
      - {_target_: detectron2.data.transforms.ResizeShortestEdge, max_size: 1333, short_edge_length: 800}
      image_format: ${...train.mapper.image_format}
      is_train: true
      use_instance_mask: true
    num_workers: 4
  train:
    _target_: detectron2.data.build_detection_train_loader
    dataset: {_target_: detectron2.data.get_detection_dataset_dicts, names: iwp_train}
    mapper:
      _target_: dataset_mappers.base_mapper.BaseMapper
      augmentations:
      - _target_: detectron2.data.transforms.RandomApply
        prob: 0.5
        tfm_or_aug:
          _target_: detectron2.data.transforms.AugmentationList
          augs:
          - _target_: detectron2.data.transforms.ResizeShortestEdge
            sample_style: choice
            short_edge_length: [400, 500, 600]
          - _target_: detectron2.data.transforms.RandomCrop
            crop_size: [384, 600]
            crop_type: absolute_range
      - _target_: detectron2.data.transforms.ResizeShortestEdge
        max_size: 1000
        sample_style: choice
        short_edge_length: [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]
      - {_target_: detectron2.data.transforms.RandomFlip, horizontal: true}
      image_format: RGB
      is_train: true
      use_instance_mask: true
    num_workers: 4
    total_batch_size: 1
lr_multiplier:
  _target_: detectron2.solver.WarmupParamScheduler
  scheduler:
    _target_: fvcore.common.param_scheduler.MultiStepParamScheduler
    milestones: [893.445, 967.8149999999999]
    num_updates: 1005
    values: [1.0, 0.1, 0.01]
  warmup_factor: 0.001
  warmup_length: 0.24875621890547264
model:
  _target_: models.rcnn.GeneralizedRCNN
  backbone:
    _target_: models.simple_feature_pyramid.SimpleFeaturePyramid
    freeze_backbone: true
    img_size: ${.net.img_size}
    in_feature: ${.net.out_feature}
    net:
      _target_: detectron2.modeling.ViT
      depth: 12
      drop_path_rate: 0.0
      embed_dim: 768
      img_size: 1024
      in_chans: 3
      mlp_ratio: 4
      norm_layer: !!python/object/apply:functools.partial
        args: [&id001 !!python/name:torch.nn.modules.normalization.LayerNorm '']
        state: !!python/tuple
        - *id001
        - !!python/tuple []
        - {eps: 1.0e-06}
        - null
      num_heads: 12
      out_feature: last_feat
      patch_size: 16
      qkv_bias: true
      residual_block_indexes: []
      window_block_indexes: [0, 1, 3, 4, 6, 7, 9, 10]
      window_size: 14
    norm: LN
    out_channels: 256
    patch_size: ${.net.patch_size}
    scale_factors: [4.0, 2.0, 1.0, 0.5]
    square_pad: 1024
    top_block: {_target_: detectron2.modeling.backbone.fpn.LastLevelMaxPool}
  input_format: RGB
  pixel_mean: [122.46, 90.9, 108.14]
  pixel_std: [33.94, 31.38, 28.77]
  proposal_generator:
    _target_: detectron2.modeling.proposal_generator.RPN
    anchor_generator:
      _target_: detectron2.modeling.anchor_generator.DefaultAnchorGenerator
      aspect_ratios: [0.5, 1.0, 2.0]
      offset: 0.0
      sizes:
      - [32]
      - [64]
      - [128]
      - [256]
      - [512]
      strides: [4, 8, 16, 32, 64]
    anchor_matcher:
      _target_: detectron2.modeling.matcher.Matcher
      allow_low_quality_matches: true
      labels: [0, -1, 1]
      thresholds: [0.3, 0.7]
    batch_size_per_image: 256
    box2box_transform:
      _target_: detectron2.modeling.box_regression.Box2BoxTransform
      weights: [1.0, 1.0, 1.0, 1.0]
    head:
      _target_: detectron2.modeling.proposal_generator.StandardRPNHead
      conv_dims: [-1, -1]
      in_channels: 256
      num_anchors: 3
    in_features: [p2, p3, p4, p5, p6]
    nms_thresh: 0.7
    positive_fraction: 0.5
    post_nms_topk: [1000, 1000]
    pre_nms_topk: [2000, 1000]
  roi_heads:
    _target_: detectron2.modeling.StandardROIHeads
    batch_size_per_image: 512
    box_head:
      _target_: detectron2.modeling.roi_heads.FastRCNNConvFCHead
      conv_dims: [256, 256, 256, 256]
      conv_norm: LN
      fc_dims: [1024]
      input_shape: !!python/object:detectron2.layers.shape_spec.ShapeSpec {channels: 256, height: 7, stride: null, width: 7}
    box_in_features: [p2, p3, p4, p5]
    box_pooler:
      _target_: detectron2.modeling.poolers.ROIPooler
      output_size: 7
      pooler_type: ROIAlignV2
      sampling_ratio: 0
      scales: [0.25, 0.125, 0.0625, 0.03125]
    box_predictor:
      _target_: detectron2.modeling.FastRCNNOutputLayers
      box2box_transform:
        _target_: detectron2.modeling.box_regression.Box2BoxTransform
        weights: [10, 10, 5, 5]
      input_shape: !!python/object:detectron2.layers.shape_spec.ShapeSpec {channels: 1024, height: null, stride: null, width: null}
      num_classes: ${..num_classes}
      test_score_thresh: 0.05
    mask_head:
      _target_: detectron2.modeling.roi_heads.MaskRCNNConvUpsampleHead
      conv_dims: [256, 256, 256, 256, 256]
      conv_norm: LN
      input_shape: !!python/object:detectron2.layers.shape_spec.ShapeSpec {channels: 256, height: 14, stride: null, width: 14}
      num_classes: ${..num_classes}
    mask_in_features: [p2, p3, p4, p5]
    mask_pooler:
      _target_: detectron2.modeling.poolers.ROIPooler
      output_size: 14
      pooler_type: ROIAlignV2
      sampling_ratio: 0
      scales: [0.25, 0.125, 0.0625, 0.03125]
    num_classes: 1
    positive_fraction: 0.25
    proposal_matcher:
      _target_: detectron2.modeling.matcher.Matcher
      allow_low_quality_matches: false
      labels: [0, 1]
      thresholds: [0.5]
optimizer:
  _target_: torch.optim.AdamW
  betas: [0.9, 0.999]
  lr: 1.25e-05
  params:
    _target_: detectron2.solver.get_default_optimizer_params
    base_lr: ${..lr}
    lr_factor_func: !!python/object/apply:functools.partial
      args: [&id002 !!python/name:detectron2.modeling.backbone.vit.get_vit_lr_decay_rate '']
      state: !!python/tuple
      - *id002
      - !!python/tuple []
      - {lr_decay_rate: 0.7, num_layers: 12}
      - null
    overrides:
      pos_embed: {weight_decay: 0.0}
    weight_decay_norm: 0.0
  weight_decay: 0.1
train:
  amp: {enabled: true}
  checkpointer: {max_to_keep: 100, period: 1000}
  ddp: {broadcast_buffers: false, find_unused_parameters: false, fp16_compression: true}
  device: cuda
  eval_period: 200
  init_checkpoint: detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True
  log_period: 20
  max_iter: 1005
  output_dir: results/clowder_mask_rcnn_vitdet/iwp
  seed: 1129
